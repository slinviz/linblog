---
title: "TKDE"
#description: <descriptive text here>
date: 2021-03-07T19:47:48+08:00
draft: true
#toc: false
#image: ""
tags: []
categories: []
---
## 背景
迭代机器学习算法通常需要海量的训练数据对模型进行多次迭代训练，最终才可能获得可以接受的模型。
显然，随着模型的逐步训练，训练集中只有越来越少的样本对模型参数更新产生影响，直接体现就是loss越来越小，
或者说小批量随机梯度下降算法中用于模型参数更新的梯度值越来越小，最后趋近于0。
也就是说此时训练集中很大一部分训练样本对模型参数更新的影响已经很小，甚至于没有影响。因此在迭代训练中
依然对其进行forward-backward操作其实是在浪费计算资源，如果有合适的方法将这部分数据样本移除，那么就
可以显著减少计算负载，减少模型训练的时间。


## 核心思想
删除对模型参数更新影响小的训练样本，选择对模型更新影响较大的部分训练数据进行模型训练，即减少每轮迭代的训练数据。

## 关键问题
1. 如何度量某个训练样本对模型更新的影响？
2. 如何高效的识别和删除对模型参数更新影响小的数据样本？

## 解决方法
> 1. 如何度量某个训练样本对模型更新的影响？

解决方案：将相似的训练样本聚集到一起，生成单个特别的样本（聚合数据点），用这个生成的样本对模型参数更新的影响来近似代表着些相似的
训练样本对模型参数更新的影响。比如10000个训练样本，之前对每个样本分别计算其影响值，现在将100个相似的样本放在一起，生成
100个聚合数据点，这样就只用算着100个聚合点的影响值就可以了。

注意：生成的聚合数据点只用于近似计算影响值，而不会参与到实际的模型训练过程中。
还有就是对聚合数据点的生成和使用应该尽可能的快，防止增加额外的开销。

图像分类中，将图像的像素展开成一个向量，然后使用Incremental SVD进行降维，降维后使用 LSH，局部敏感哈希进行划分，
最后对划分在同一组中的特征求均值，得到均值向量作为聚合数据点的特征向量。

在Spark平台下，可以将具有相同的标签的数据划分为一个RDD的Partition，然后并行的对每个Partition计算聚合数据点，最后在进行合并。

> 2. 如何高效的识别和删除对模型参数更新影响小的数据样本？

关键：影响值的计算，可以选用loss也可以选用梯度的绝对值之和。
然而计算梯度的开销较大，而且最后的结果跟用loss计算的结果相差不大。

阈值：判断冗余数据的阈值：1. 设定百分比，比如50% 则删除50%的数据。 2. 将影响值从小到大排序，将累计和小于总和的1%的前x个样本作为
冗余数据删除。

在BigDL中，每个Executor复杂处理一个Partition的训练样本，而根据每个Executor分配的CPU核数用多线程进行计算。
存在的问题是某些线程可能会删除更多的数据，因此最后需要将各个线程筛选过后的样本聚合后重新平均地划分到各个线程中，
以减少出现计算快的线程需要等待慢的线程很久。

### 核心集：构建比原数据集更小的数据集来近似代表原数据集
### 重要性采样：根据梯度值对训练样本进行重要性采样
